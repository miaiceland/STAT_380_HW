---
title: "STAT380_HW6"
author: Mia Iceland
output: html_document
date: "Due: Thursday, March 14, 2024 by 11:59 PM"
---

## Instructions

In class, we covered the basics of using k-fold cross validation for multiple linear regression models and kNN regression models. The activities that follow are meant to provide further practice with these concepts. NOTE: All required `library` commands **must be** included in the Front Matter section. If you include your `library` commands elsewhere in your code, you will be penalized.

At the conclusion to the activity, you should upload

1. your .html file named LastnameFirstInitial_STAT380_HW6.html 
2. your .Rmd file named LastnameFirstInitial_STAT380_HW6.Rmd

## Learning Objectives
This assignment address aspects of the following learning objectives.

1. Students will be able to load datasets from R packages and external sources into the R environment.
2. Students will be able to identify R functions for and perform data wrangling tasks such as filtering observations based on a criterion, creating new variables, and restructuring data.
3. Students will be able to use iteration (loops, apply, etc.) to perform tasks that must be repeated many times on a given dataset.
4. Given a statistical learning method, students will be able to restructure the data for use in the corresponding R function.
5. Fit a linear regression model using statistical software, including situations involving polynomial terms, categorical predictions, and/or interaction terms.
6. Perform k-nearest neighbors regression using statistical software, including using cross validation to select the number of neighbors.
7. Compare the ability of competing models to generalize for new data.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter - Clean Environment, Load Libraries, User Defined Functions
```{r}
#Clear Objects from Environment
remove(list = ls())

#Load Libraries
library(tidyverse)
library(FNN) #Use FNN for knn regression

# Download Data set
Ins <- read.csv("~/Downloads/L02_Insurance_m.csv")

```


## Dataset

`L02_Insurance_m.csv` contains information about a number of health insurance policies. In particular, the data set contains some attributes of the policy holder (such as age, sex, etc.) and the total charges billed by the health care provider. Details about the variables included follow.

* `age` - age of primary beneficiary
* `sex` - sex of primary beneficiary
* `bmi` - Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m \^ 2) using the ratio of height to weight, ideally 18.5 to 24.9
* `children` - Number of children covered by the health insurance policy (i.e., the number of dependents)
* `smoker` - Status indicating whether the person is a smoker (options include 'yes' and 'no')
* `region` - the beneficiary's residential area in the US (options include northeast, southeast, southwest, northwest).
* `charges` - Individual medical costs as billed by health insurance

Our goal is to predict `charges` using `age`, `sex`, `bmi`, `children`, and smoking status (`smoker`).

## Problem 1 - Which method (multiple linear regression or kNN) produces the most accurate predictions for new data? To answer this, compare the performance of the methods using 10-fold cross validation.

a. Prepare the data for implementing multiple linear regression and kNN. NOTE: I am not telling you the exact steps to take prepare the data, but if you unsure of the steps, see Problem 2 in HW5. 

```{r}
#Remove rows with 'NA' from the data set
Ins <-
  na.omit(Ins)

#Create indicator variables
Ins <- 
  Ins %>%
  mutate(smokes = ifelse(smoker == "yes", 1, 0),
         male = ifelse(sex == "male", 1, 0))

#Standardize the data so that each input has a mean of 0 and standard deviation of 1
xvars <- c("age", "male", "bmi", "children", "smokes")
Ins[ , xvars] <- scale(Ins[ , xvars], center = TRUE, scale = TRUE)

#Randomly select 80% of the row numbers to determine which rows are assigned to training set
set.seed(123)
train_ind <- sample(1:nrow(Ins), floor(0.8*nrow(Ins)))
set.seed(NULL)

Train <- Ins[train_ind, ]
Validation <- Ins[-train_ind, ]

```


b. Assign the fold values and randomly permute them using a seed of 123. To make sure you are on the right path, the folds for the first few observations are provided below. (If you cannot see the results, check the .html file containing the knitted instructions.)

```{r}
#Create fold values
num_folds <- 10
folds <- cut(x = 1:nrow(Ins), breaks = num_folds, labels = FALSE)

#Randomly permute fold values
set.seed(123)
folds <- sample(folds) #randomly permute the fold values
set.seed(NULL)

```


c. Implement 10-fold cross validation for the multiple linear regression model for predicting `charges` using `age`, `sex`, `bmi`, `children`, and smoking status (`smoker`). For each fold, calculate the Mean Square Error (MSE). Display the 10 MSE values **and** report the mean of the 10 MSE values.

    * NOTE: Be sure to compute the MSE for each fold instead of the RMSE for each fold.
    * NOTE: To implement 10-fold cross validation for this problem (and the next problem), you should write a loop for cycling through the folds as discussed in class instead of using a black box function that does everything behind the scenes.

```{r}
#Initialize
mseVec <- rep(NA, num_folds)

#Loop for controlling the folds
for(i in 1:num_folds){
  #Training/Validation Split
  valInd <- which(folds == i)
  Validation <- Ins[valInd, ]
  Train <- Ins[-valInd, ]

#Build Model on Train
tempModel <- lm(charges ~ age + male + bmi + children + smokes, data = Train)

#Obtain Predictions for Validation Set
yhat <- predict(tempModel, newdata = Validation)

#Calculate and Store MSE Values
mseVec[i] <- mean((Validation$charges - yhat)^2)

}

#10 MSE Values
mseVec

#Displaying the MSE value
mean(mseVec)

#Displaying the RMSE value
sqrt(mean(mseVec))

```


d. Implement 10-fold cross validation for the k-Nearest Neighbors (kNN) regression model for predicting `charges` using `age`, `sex`, `bmi`, `children`, and smoking status (`smoker`). For the purposes of selecting k, consider values of k ranging from 1 to 50. Create a plot showing the Mean Mean Square Error (Mean MSE) for each fold as a function of k **and** state which value of k is optimal.

     * NOTE: This can be challenging since we did not do this exact problem in the notes. The Lecture 10 Supplement document outlines the procedure and the code for you. You should start by reading the document.

```{r}
#Initialize
maxK <- 75
mseVec <- rep(NA, maxK)
rmseVec <- rep(NA, maxK)

#Set up storage for MSE values in a matrix
mseMat <-
  matrix(NA, nrow = num_folds, ncol = maxK)

# Nested Loop
for (i in 1:num_folds){
  
  valInd <- which(folds == i)
  Validation <- Ins[valInd, ]
  Train <- Ins[-valInd, ]

for (j in 1:maxK){

  #Build Model
  knnRes <- knn.reg(train = Train[ , xvars, drop = FALSE],
                   test = Validation[ , xvars, drop = FALSE],
                   y = Train$charges,
                   k = j)

  #Calculate and Store MSE Values
  mseMat[i,j] <-
    mean((Validation$charges - knnRes$pred)^2)

  }
}

#Displaying the k value of the minimum MSE
which.min(apply(mseMat, 2, mean))

#Displaying the minimum MSE
meanMSE <- apply(mseMat, 2, mean)
min(meanMSE)

#Displaying the minimum RMSE
sqrt(min(meanMSE))

#Finding the SD and creating storage data frame so we can use ggplot
sdMSE <- apply(mseMat, 2, sd)
tempDF <- data.frame(k = 1:maxK, mse = meanMSE, sd = sdMSE)

#Create plot
ggplot(data = tempDF, mapping = aes(x = k, y = mse)) +
  geom_line() +
  labs(y = "MSE", x = "k")

```


e. Which method (multiple linear regression or kNN) produces the most accurate predictions for new data? **Explain your reasoning** by discussing the RMSE values associated with each method. NOTE: Although we calculated MSE values in Parts c. and d., compare the RMSE values in this question. Be sure to state the RMSE values.

The RMSE value using multiple linear regression is 6075.479 while the RMSE value using 10-fold cross validation kNN is 4944.097. The method that produces the most accurate results is the 10-fold cross validation kNN because its RMSE value is lower than the multiple linear regression model. RMSE, square root mean standard error, determines the average error of the model compared to the actual data. If the RMSE is low, the results are more accurate. If the RMSE is higher, there is greater error. Since 6075.479 is greater than 4944.097, 10-fold cross validation kNN has the smallest RMSE value, therefore, the best fit model. 

## Problem 2 - Plot
Recreate the plot shown below. (If you are not seeing the plot, be sure to open the .html file containing the instructions from Canvas.) The plot shows the plot created in Problem 1d (MSE as a function of k), but includes errors bars. For each value of k, you are to draw a **red** line that goes from  (Mean MSE - standard deviation) to (Mean MSE + standard deviation). The Mean MSE is the mean of the MSE values within each fold. The standard deviation in this problem corresponds to the standard deviation of the MSE values within each fold. NOTE: While the error bars appear to have a similar length, the standard deviation is a different number for each value of k. 

```{r}
#Finding MSE and SD
meanMSE <- apply(mseMat, 2, mean)
sdMSE <- apply(mseMat, 2, sd)

#Create storage data frame so we can use ggplot
tempDF <- data.frame(k = 1:maxK, mse = meanMSE, sd = sdMSE)

#Create plot
ggplot(data = tempDF, mapping = aes(x = k, y = mse)) +
  geom_errorbar(aes(ymin = mse - sd, ymax = mse + sd), 
                width = 0.1,  
                color = "red") +
  geom_line() +
  labs(y = "MSE", x = "k")
```

